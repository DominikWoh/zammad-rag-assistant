version: "3.9"

services:
  rag_app:
    image: python:3.11-slim
    container_name: rag_app
    working_dir: /app
    command: ["bash","-lc","chmod +x /app/tools/docker/start.sh && /app/tools/docker/start.sh"]
    volumes:
      - ../../:/app:rw
      - ./logs:/var/log/rag:rw
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "5000:5000"
    env_file:
      - ./.env
    environment:
      - PYTHONUNBUFFERED=1
      - DEPLOY_MODE=docker
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3:8b}
    depends_on:
      - qdrant
      - ollama
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./data/qdrant:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./data/ollama:/root/.ollama
    entrypoint: ["/bin/sh","-lc"]
    command: |
      '
      # Starte den Ollama-Dienst im Hintergrund
      ( /usr/bin/ollama serve & ) ;
      # Warte bis API erreichbar ist
      echo "Warte auf Ollama API..."
      for i in $(seq 1 60); do
        if curl -sf http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then
          echo "Ollama API erreichbar."
          break
        fi
        sleep 1
      done
      # Modell automatisch pullen (idempotent)
      MODEL="${OLLAMA_MODEL:-qwen3:8b}"
      echo "Pulle Modell: ${MODEL}"
      /usr/bin/ollama pull "${MODEL}" || true
      # Halte den Container im Vordergrund
      wait
      '
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3:8b}
    restart: unless-stopped